# 2.7 Lab 2 
#### Welcome to my second lab! For this lab, I worked with information from animal crossing on different colored socks. The purpose of this lab was to access data using a web API and contemplate how using classes can help us analyze datasets. This lab was also a review of using python methods to analyze data. I hope that you enjoy!

![Animal Crossing Photo](https://media.wired.com/photos/5e8cebbda231050008136013/125:94/w_1750,h_1316,c_limit/animal-crossing-history-wired.jpg)
[Image source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.wired.com%2Fstory%2Funbearable-lightness-animal-crossing%2F&psig=AOvVaw2e94aUBxpMxf68bW7hTnK4&ust=1638472215218000&source=images&cd=vfe&ved=0CAsQjRxqFwoTCMDR9-Smw_QCFQAAAAAdAAAAABAe)


## Task 1 - Discuss how you used the API to obtain the dataset.

In order to analyze the socks, we first had to create the dataset, or csv. In order to do so, I used an API to access this data online. I think that there was one main difficult component of accessing the data online. In order to determine  how to loop through my data, because I needed to figure out how to determine when we had cycled through every single sock. Initially, I thought about hardcoding it. This would have entailed counting up the number of socks in the dataset and looping through the dataset this number of times. The pseudocode I wrote for that is the following:

```python:
#for i in range 0, number of socks:
    #BASE_URL = "https://hm-cs.herokuapp.com"
    #ENDPOINT = "/socks"
    #API_key= "ArtOfDataKEY123"
    #payload = {"key": API_key, "idx": str(i) }
    #response = requests.get(BASE_URL+ENDPOINT,params=payload)
    #print response 
    #add one to i
```
The key line here was payload = {"key": API_key, "idx": str(i) } , because I had to figure out that the index was the value i. 

However, while this would work for this dataset, I wanted to generalize this code so that it could work for any dataset. To do so, Tuhin and I thought about alternative methods we could use to determine when we had finished looping through all of the different socks. Ultimately, we realized that if we tried to index to a sock that didn't exist, the response would be invalid. So, instead of thinking about viable indicies, we looked at viable responses. That was when we realized we could use the line

```python:
while response.ok:
```
instead of the for loop. With the while loop, we still used a counter i to loop through all the indicies, but we no longer needed to hardcode the explicit number of indicies into the code. 

After figuring out how to access the dataset with APIs, I then decided to print the response using this line:

```python
  print(response.text)
```
This way, I could use redirection in my terminal to print each of the sock variations directly into a new csv, which I could then analyze for the next questions. 


## Task 2 - Which sock has the most variations? If there is more than one answer, then list all of them.

For me, this task felt like a nice review of dictionaries. In particular, this task reminded me a lot of the penguins lab where we wanted to tally the number of penguins that belonged to each island. To start, I created a method that would intake the data from the csv that I had just created. Then, I created a table that would be an empty dictionary, where each key  was a sock's name and the value was the number of socks that had the same name. This value is equivalent to the number of variations of each sock, because all of the socks with the same name are the different variations of that type of sock. In order to create this table, I iterated through each sock in the datset. Then, I iterated through this table to find the sock types with the most variations. I think that this was definitely the trickiest part of the task for me. Initially, I thought that I would make an empty list, and every time I encountered a socks type with a higher variation than the current highest variation, I would append it to the list. My pseudocode for that looked like this:
```python
    # most_frequent_type = []  is the empty list 
    #largest_number_times_appeared = 1 
    #for name in table: 
       # if name's value is greater than or equal to largest_number_times_appeared: 
            #set largest_number_times_appeared to the value
            # add that variation to the most_frequent_type list
    #return the list

```

However, I quickly found that this code erred in a key way: if the current largest value is updated to a larger number, the list still contains all of the sock types with the former largest value even. I realized that every time the largest value was updated, the list either had to remove all current sock variations, or I had to make a new list entirely. I decided to make a new list entirely, so I added the following line of code:

```python
if table[name] > largest_number_times_appeared: 
            largest_number_times_appeared = table[name] 
            most_frequent_type = []
            most_frequent_type.append(name)
```

The new line I wrote was the most_frequent_type = [], which basically creates a new empty list every time the loop comes across a sock type with a larger number of variations. Then I used a format string to print out the method's findings :)

## Task 3 - How many socks of each color are there? If a sock has two different colors, it should be counted in both. However, if a sock has the same Color1 and Color2, make sure you donâ€™t double count it! 

This task drew on a lot of the same skills as the second task. Like the second task, my first steps were to intake the data and create an empty table called table_color. This way, I could loop through each sock in the dataset and store it's first and second colors in variables. Then, I added the first color into the dictionary if it was not already in there, and added one to its value. Then, I had to check to see if the second color was different than the first color. If it was different, I added it to the dictionary. Then I returned the table. Here was my pseudocode:


```python
#intake data
#table_color = {}
#for each sock in the dataset:
    #store first color in Color_1
    #store second color in Color_2
    #if Color_1 is not in table_color:
        #make that color a key with value = 0
    #add one to the value of Color_1
    #if Color_1 is not Color_2
        #if Color_2 is not in dictionary, add it
        #add 1 to table_color[Color_2]
    #return table_color
```
While this code felt structurally correct, it kept giving me the wrong answer. Ultimately, Tuhin and I realized that this was because I had written this line:

```python
if Color_2 is not Color_1:
```
as opposed to this line:

```python
if Color_2 != Color_1:
```
The issue was that the "is not" compares strings, while here I was comparing integers. With this small fix, my code worked!



## Concluding thoughts
Ultimately, this lab helped me solidfy ny understanding of how APIs work. Before this lab, I did not understand what the payload was and how we can manipulate it to access the different data we are interested in using. It was also a neat example of how we can querry information to make our own dataset or csv. Moreover, the response.ok line is super useful, and I will be sure to look for instances where I can use it in the future. 

One thing I wish I could have done in this lab was use classes. I did create a sock class in my code (that I left there so that I can come back to this when I have more time in the future!), however each time I attempted to use it, I found my code just got more complicated than it had to be. Although I did not have the time to do it before I turn in this lab, I plan on meeting up with Uddpito and taking a look at his code to see how I could implement classes more effectively to help me complete these tasks. I don't think I fully understand how I could use them effectively yet, and I'm eager and excited to learn.

I hypothesize that I could have used classes to help me compare  different sock types directly, possibly without having to make different tables and dictionaries and loop through them as many times. If this is true, then classes would definitely have been a smart and effective way of analyzing this dataset!